{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.jit import Final\n",
    "import torch.nn.functional as F\n",
    "from timm.layers import use_fused_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanira Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            num_heads: int = 8,\n",
    "            qkv_bias: bool = False,\n",
    "            qk_norm: bool = False,\n",
    "            attn_drop: float = 0.,\n",
    "            proj_drop: float = 0.,\n",
    "            norm_layer: nn.Module = nn.LayerNorm,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([4, 257, 384])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "dim = 384  # Dimension of the input features\n",
    "num_heads = 6  # Number of attention heads\n",
    "batch_size = 4  # Number of samples in a batch\n",
    "tokens = 257  # Length of the input sequences\n",
    "\n",
    "# Create a random input tensor\n",
    "input_data = torch.randn(batch_size, tokens, dim)\n",
    "\n",
    "# Initialize the attention layer\n",
    "att = Attention(dim=dim, num_heads=num_heads)\n",
    "\n",
    "# Run the forward pass\n",
    "output_tensor = att(input_data)\n",
    "\n",
    "print(\"Output tensor shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid_Attention(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            num_heads: int = 8,\n",
    "            qkv_bias: bool = False,\n",
    "            qk_norm: bool = False,\n",
    "            attn_drop: float = 0.,\n",
    "            proj_drop: float = 0.,\n",
    "            norm_layer: nn.Module = nn.LayerNorm,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        # Custom sigmoid attention calculation\n",
    "        print(torch.matmul(q, k.transpose(-2, -1))[0][0][0][:10])\n",
    "        sigmoid_att = torch.sigmoid(torch.matmul(q, k.transpose(-2, -1)))\n",
    "        print(sigmoid_att[0][0][0][:10])\n",
    "        sigmoid_att[0][0][0][2] = 0.8\n",
    "        print(sigmoid_att[0][0][0][:10])\n",
    "        inverse_sigmoid_att = torch.log(sigmoid_att / (1 - sigmoid_att))\n",
    "        print(inverse_sigmoid_att[0][0][0][:10])\n",
    "        attn = F.softmax(inverse_sigmoid_att / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)), dim=-1)\n",
    "\n",
    "        # Apply dropout to attention\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # Compute output\n",
    "        x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4186, -1.2876,  2.0219, -2.0418, -1.5648,  2.4395, -0.2220,  3.8145,\n",
      "        -0.1355, -1.0452], grad_fn=<SliceBackward0>)\n",
      "tensor([0.0818, 0.2163, 0.8831, 0.1149, 0.1730, 0.9198, 0.4447, 0.9784, 0.4662,\n",
      "        0.2602], grad_fn=<SliceBackward0>)\n",
      "tensor([0.0818, 0.2163, 0.8000, 0.1149, 0.1730, 0.9198, 0.4447, 0.9784, 0.4662,\n",
      "        0.2602], grad_fn=<SliceBackward0>)\n",
      "tensor([-2.4186, -1.2876,  1.3863, -2.0418, -1.5648,  2.4395, -0.2220,  3.8145,\n",
      "        -0.1355, -1.0452], grad_fn=<SliceBackward0>)\n",
      "torch.Size([4, 257, 384])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "dim = 384  # Dimension of the input features\n",
    "num_heads = 6  # Number of attention heads\n",
    "batch_size = 4  # Number of samples in a batch\n",
    "tokens = 257  # Length of the input sequences\n",
    "\n",
    "attention = Sigmoid_Attention(dim=dim, num_heads=num_heads)\n",
    "x = torch.randn(batch_size, tokens, dim)\n",
    "output = attention(x)\n",
    "print(output.shape)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
